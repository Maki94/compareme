\section{Linearna diskriminentna analiza - LDA}

Logistička regresija uključuje direktno modeliranje $Pr\left(Y=k | X = x\right)$
koristeći logističku funkciju (\ref{eq:logistic_function}) za slučaj binarnog
izlaza, pored ovog nedostatka javlja se i problem nestabilnosti kod dobro
odvojenih klasa, ovo nije slučaj sa LDA. Sa druge strane LDA je stabilniji
klasifajer kada je broj prediktora $X$ mali i ima približno normalnu
distribuciju. \\

LDA ima manje direktan pristup od logističke regresije. Naime, procena
verovatnoće se vrši uz modeliranjem distribucije prediktora X odvojeno za svaku
od rezltujućih klasa $Y$, i nakon toga primenom Bajesove teoreme za
prebacivanje ovih procena u verovatnoću $Pr\left(Y=k | X = x\right)$. Kada su
ove distribucije normalne onda je model sličan logističkoj regresiji. \\

\subsection{Bayes theorem fro classification}

Pretpostavimo da želimo da klasifikujemo podatke u $k$ klasa, $k \geq 2$. Sada
kvantitivni izlaz $Y$ može imati  $k$ različitih vrednosti. Uzmimo da $\pi_k$
predstavlja verovatnoću da posmatranje $x_i$ pripada klasi k, i da
$f_k\left(X\right) \equiv Pr\left(X=x | Y=k\right)$ označava funkciju gustine od $X$ da jedno
posmatranje pripada klasi $k$. Prema Bajesovoj teorimi:
\begin{equation} \label{eq:lda_bayes}
  p_k\left(x\right) = Pr(X=x | Y=k) =
  \frac
    {\pi_k f_k\left(x\right)}
    {\sum_{l=1}^{K}\pi_l f_l\left(x\right)}
\end{equation}
$\pi_k$ predstavlja frakciju trening podataka koji pripadaju trening skupu, dok
je procena $f_k\left(X\right)$ malo zahtevnija. Procenom ove funkcije dobijamo
klasifajer koji procenjuje Bajesov klasifajer.

\subsection{LDA for p = 1}
Pretpostavimo da imamo samo jedan prediktor. Želimo da procenimo
$f_k\left(x\right)$ kako bismo uz pomoć (\ref{eq:lda_bayes}) odredili
$p_k\left(x\right)$ i klasifikovali podataka $x$ određenoj klasi za koju je
$p_k\left(x\right)$ najveći. Kako bismo procenili $f_k\left(x\right)$,
napravićemo par pretpostavki. \\

Smatrajmo da $f_k\left(x\right)$ ima Gausovu raspodelu. U jedno dimenzionalnim
uslovima normalna gustina ima oblik:
\begin{equation} \label{eq:lda_gaussian}
  f_k\left(x\right) =
    \frac {1} {\sqrt{2\pi}\sigma_k}
    e^{-\frac{1}{2 \sigma_k^2} (x - \mu_k)^2}
\end{equation}
gde je $\mu_k$ srednja vrednost, a $\sigma_k^2$ varijansa za $k$-tu klasu. Zatim
smatrajmo da imamo istu varijansu za svaku klasu
($\sigma^2 = \sigma_1^2 = \sigma_2^2 = \cdot\cdot\cdot = \sigma_k^2$). Iz
(\ref{eq:lda_bayes}) i (\ref{eq:lda_gaussian}) dobijamo:
\begin{equation} \label{eq:lda_p_k_normal}
  p_k\left(x\right) =
    \frac
    {\pi_k \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2 \sigma^2} (x - \mu_k)^2}}
    {\sum_{l=1}^{K} \pi_l \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2 \sigma^2} (x - \mu_l)^2}}
\end{equation}
Bajesov klasifajer dodeljuje jednom podatku $X = x$ klasu za koji je
(\ref{eq:lda_p_k_normal}) najveći. Logaritmovanjem (\ref{eq:lda_p_k_normal}) i
sređivanjem izraza dobijamo ekvivalentan zapis
\begin{equation} \label{eq:lda_delta_k_normal}
  \delta_k\left(x\right) =
    x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log{\pi_k}
\end{equation}
za koji je $\delta_k\left(x\right)$ najveći. \\

LDA približno određuje Bajesov klasifajer kombinovanjem procenjeni vrednosti ya:
$\pi_k$, $\mu_k$ i $\sigma^2$ i jednačinu (\ref{eq:lda_delta_k_normal}). LDA
vrši procenu:
\begin{equation} \label{eq:lda_mu} \hat{\mu} = \frac{1}{n_k} \sum_{i:y_i=k} x_i \end{equation}
\begin{equation} \label{eq:lda_sigma} \hat{\sigma}^2 = \frac{1}{n - K} \sum_{k=1}^{K} \sum_{i:y_i=k} (x_i-\hat{\mu_k})^2 \end{equation}
\begin{equation} \label{eq:lda_pi} \hat{\pi_k} = \frac{n_k}{n} \end{equation}
gde je $n$ ukupan broj trening podataka, $n_k$ broj trening podataka koji
pripadaju klasi $k$, $\mu_k$ srednja vrednost svih trening podataka iz klase $k$,
a $\sigma$ težinska srednja vrednost varijanse za svaku od K klasa. \\

Integrisanjem (\ref{eq:lda_mu}), (\ref{eq:lda_sigma}) i (\ref{eq:lda_pi}) u
(\ref{eq:lda_delta_k_normal}) i dodteljivanjem klase ulaznom podataku $X = x$ za
koji je:
\begin{equation} \label{eq:lda_delta_k_lda}
  \hat{\delta_k}\left(x\right) =
    x\frac{\hat{\mu_k}}{\hat{\sigma}^2} - \frac{\hat{\mu_k}^2}{2\hat{\sigma}^2} + \log{\hat{\pi_k}}
\end{equation}
najveći. Naziv linearan u LDA upravo potiče iz činjenice da je diskrimitivna
funkcija $\hat{\delta_k}\left(x\right)$ u (\ref{eq:lda_delta_k_lda}) linearna od
$x$.

\subsection{For p > 1}

Za veći broj prediktora $X = (X_1, X_2, \cdot\cdot\cdot, X_p)$ primenićemo
multivariacionu normalnu distribiciju sa specifičnim vektorom srednjih vrednosti
i zajedničku matricu kovarjansi.

P-dimenzionalni vektor $X$ ima multivariacionu normalnu distribiciju i obeležavamo
sa $X \sim N(\mu, \sum)$, srednja vrednost od X je $E(X)=\mu$ i $p$ x $p$ matrica
kovarjansi $Cov(X)=\sum$. Za ove parametre dobijamo multivariacionu Gausovu funkciju gustine:
\begin{equation} \label{eq:lda_multi_var_gauss_density}
  f(x) =
  \frac{1}{(2\pi)^{\pi/2}|\sum|^{1/2}}
  e^{-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)}
\end{equation}
sada, integrisanjem ove funkcije gustine u (\ref{eq:lda_bayes}) dobijamo funkciju
čijem ulazu $x$ vrednost je klasu za koju je vrednost:
\begin{equation} \label{}
  \delta_k\left(x\right) =
    x^T\sum^{-1}\mu_k -\frac{1}{2}\mu_k^T\sum^{-1}\mu_k + \log{\pi_k}
\end{equation}
najveća.

\subsection{QDA}
